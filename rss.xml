<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Gatsby Starter Blog RSS Feed]]></title><description><![CDATA[Human-AI Interaction Lab at Kangwon National University. Research in HCI, Ubiquitous Computing, Machine Learning & Mobile Sensing.]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 11 Aug 2025 06:45:52 GMT</lastBuildDate><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/han-mingyu/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/han-mingyu/</guid><content:encoded></content:encoded></item><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/hwang/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/hwang/</guid><content:encoded></content:encoded></item><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/kim-minji/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/kim-minji/</guid><content:encoded></content:encoded></item><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/kim-minyoung/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/kim-minyoung/</guid><content:encoded></content:encoded></item><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/kim/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/kim/</guid><content:encoded></content:encoded></item><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/lee-jiwook/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/lee-jiwook/</guid><content:encoded></content:encoded></item><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/lee/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/lee/</guid><content:encoded></content:encoded></item><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/son/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/son/</guid><content:encoded></content:encoded></item><item><title><![CDATA[No title]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/sunatlaev/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/sunatlaev/</guid><content:encoded></content:encoded></item><item><title><![CDATA[Vietnam Ho Chi Minh]]></title><description><![CDATA[News about research activities and academic exchange in Ho Chi Minh, Vietnam.]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/vietnam-hochiminh-trip/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/vietnam-hochiminh-trip/</guid><pubDate>Thu, 10 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;News about research activities and academic exchange in Ho Chi Minh, Vietnam.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Yokohama CHI 2025]]></title><description><![CDATA[News about conference participation and research presentation at CHI 2025 in Yokohama, Japan.]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/yokohama-chi2025-trip/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/yokohama-chi2025-trip/</guid><pubDate>Sun, 20 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;News about conference participation and research presentation at CHI 2025 in Yokohama, Japan.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[A Dataset on Takeover During Distracted L2 Automated Driving]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Dataset-on-Takeover-During-Distracted-L2-Automated-Driving/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Dataset-on-Takeover-During-Distracted-L2-Automated-Driving/</guid><pubDate>Mon, 31 Mar 2025 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Effects of In-Vehicle Auditory Interactions on Takeover Performance in SAE L2 Semi-Automated Vehicles]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Effects-of-In-Vehicle-Auditory-Interactions/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Effects-of-In-Vehicle-Auditory-Interactions/</guid><pubDate>Sat, 01 Feb 2025 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Introduction to Data Utilization (Data Security and Utilization)]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/data-utilization/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/data-utilization/</guid><pubDate>Wed, 15 Jan 2025 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Mobile Programming]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/mobile-programming/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/mobile-programming/</guid><pubDate>Wed, 15 Jan 2025 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Python Programming]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/python-programming/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/python-programming/</guid><pubDate>Wed, 15 Jan 2025 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Web Programming]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/web-programming/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/web-programming/</guid><pubDate>Wed, 15 Jan 2025 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Sydney UbiComp 2024]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/sydney-ubicomp-2024/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/sydney-ubicomp-2024/</guid><pubDate>Tue, 08 Oct 2024 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Exploring Context-Aware Mental Health Self-Tracking Using Multimodal Smart Speakers in Home Environments]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Exploring-Context-Aware-Mental-Health-Self-Tracking/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Exploring-Context-Aware-Mental-Health-Self-Tracking/</guid><pubDate>Sat, 11 May 2024 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Interrupting for Microlearning: Understanding Perceptions and Interruptibility of Proactive Conversational Microlearning Services]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Interrupting-for-Microlearning/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Interrupting-for-Microlearning/</guid><pubDate>Sat, 11 May 2024 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[S-ADL: Exploring Smartphone-Based Activities of Daily Living to Detect Blood Alcohol Concentration in a Controlled Environment]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/S-ADL-Exploring-Smartphone-Based-Activities-of-Daily-Living/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/S-ADL-Exploring-Smartphone-Based-Activities-of-Daily-Living/</guid><pubDate>Sat, 11 May 2024 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Internet of Things-Enabled Patch with Built-in Microsensors and Wireless Chip: Real-Time Remote Monitoring of Patch Treatment]]></title><link>https://hai-lab-knu.github.io/HAI-Lab-Website/IoT-Enabled-Patch-with-Microsensors/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/IoT-Enabled-Patch-with-Microsensors/</guid><pubDate>Wed, 01 May 2024 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Interrupting for Microlearning: Understanding Perceptions and Interruptibility of Proactive Conversational Microlearning Services]]></title><description><![CDATA[This project investigates the design and implementation of proactive conversational microlearning services using smart speakers. Theâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/interrupting-for-microlearning/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/interrupting-for-microlearning/</guid><pubDate>Mon, 15 Jan 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This project investigates the design and implementation of proactive conversational microlearning services using smart speakers. The research focuses on understanding when and how to deliver bite-sized learning content without disrupting users&apos; daily activities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Research Areas:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Microlearning delivery timing&lt;/li&gt;
&lt;li&gt;User interruptibility patterns&lt;/li&gt;
&lt;li&gt;Conversational AI for education&lt;/li&gt;
&lt;li&gt;Smart speaker interaction design&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Field studies with smart speaker users&lt;/li&gt;
&lt;li&gt;Interruptibility assessment frameworks&lt;/li&gt;
&lt;li&gt;Conversational microlearning prototypes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Expected Outcomes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guidelines for timing microlearning delivery&lt;/li&gt;
&lt;li&gt;User perception insights&lt;/li&gt;
&lt;li&gt;Design recommendations for proactive learning services&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[AI Complaint Classification for University IT Services]]></title><description><![CDATA[This project investigates the application of artificial intelligence and natural language processing techniques to automatically classify ITâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/ai-complaint-classification/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/ai-complaint-classification/</guid><pubDate>Wed, 10 Jan 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This project investigates the application of artificial intelligence and natural language processing techniques to automatically classify IT service complaints in university settings. The system aims to improve the efficiency of IT support services by accurately categorizing incoming complaints and routing them to appropriate departments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Research Areas:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Natural Language Processing&lt;/li&gt;
&lt;li&gt;Text Classification&lt;/li&gt;
&lt;li&gt;IT Service Management&lt;/li&gt;
&lt;li&gt;University IT Systems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning models for text classification&lt;/li&gt;
&lt;li&gt;University IT complaint dataset analysis&lt;/li&gt;
&lt;li&gt;Automated routing system development&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Expected Outcomes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improved IT service response times&lt;/li&gt;
&lt;li&gt;Automated complaint categorization&lt;/li&gt;
&lt;li&gt;Enhanced user satisfaction&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[K-EmoPhone: A Mobile and Wearable Dataset with In-Situ Emotion, Stress, and Attention Labels]]></title><description><![CDATA[K-EmoPhone: A Mobile and Wearable Dataset with In-Situ Emotion, Stress, and Attention Labels Abstract With the popularization of low-costâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/K-EmoPhone/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/K-EmoPhone/</guid><pubDate>Fri, 02 Jun 2023 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;K-EmoPhone: A Mobile and Wearable Dataset with In-Situ Emotion, Stress, and Attention Labels&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;With the popularization of low-cost mobile and wearable sensors, several studies have used them to track and analyze mental well-being, productivity, and behavioral patterns. However, there is still a lack of open datasets collected in real-world contexts with affective and cognitive state labels such as emotion, stress, and attention; the lack of such datasets limits research advances in affective computing and human-computer interaction.&lt;/p&gt;
&lt;p&gt;This study presents K-EmoPhone, a real-world multimodal dataset collected from 77 students over seven days. This dataset contains:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Continuous probing of peripheral physiological signals and mobility data&lt;/strong&gt; measured by commercial off-the-shelf devices&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context and interaction data&lt;/strong&gt; collected from individuals&apos; smartphones&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5,582 self-reported affect states&lt;/strong&gt;, including emotions, stress, attention, and task disturbance, acquired by the experience sampling method&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Dataset Overview&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Participants&lt;/strong&gt;: 77 students&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Duration&lt;/strong&gt;: 7 days per participant&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Affect Labels&lt;/strong&gt;: 5,582 self-reported states&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Types&lt;/strong&gt;: Physiological signals, mobility data, smartphone interaction data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collection Method&lt;/strong&gt;: Experience sampling method (ESM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Key Contributions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First large-scale real-world dataset with in-situ emotion, stress, and attention labels&lt;/li&gt;
&lt;li&gt;Multimodal data collection from mobile and wearable devices&lt;/li&gt;
&lt;li&gt;Comprehensive affect state annotations through experience sampling&lt;/li&gt;
&lt;li&gt;Enables research advances in affective computing and human-computer interaction&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;This dataset addresses the critical gap in open datasets for affective computing research, providing researchers with real-world multimodal data to advance understanding of human emotional and cognitive states in naturalistic settings.&lt;/p&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Kang, S., Choi, W., Park, C.Y. et al. K-EmoPhone: A mobile and wearable dataset with in-situ emotion, stress, and attention labels. Sci Data 10, 351 (2023). &lt;a href=&quot;https://doi.org/10.1038/s41597-023-02253-5&quot;&gt;https://doi.org/10.1038/s41597-023-02253-5&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Endpoint Device Risk-Scoring Algorithm Proposal for Zero Trust]]></title><description><![CDATA[Endpoint Device Risk-Scoring Algorithm Proposal for Zero Trust Abstract The rapid expansion of remote work following the COVID-19 pandemicâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Endpoint-device-risk-scoring/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Endpoint-device-risk-scoring/</guid><pubDate>Tue, 18 Apr 2023 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Endpoint Device Risk-Scoring Algorithm Proposal for Zero Trust&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The rapid expansion of remote work following the COVID-19 pandemic has necessitated the development of more robust and secure endpoint device security solutions. Companies have begun to adopt the zero trust security concept as an alternative to traditional network boundary security measures, which requires that every device and user be considered untrustworthy until proven otherwise.&lt;/p&gt;
&lt;p&gt;Despite the potential benefits of implementing zero trust, the stringent security measures can inadvertently lead to low availability by denying access to legitimate users or limiting their ability to access necessary resources. To address this challenge, we propose a risk-scoring algorithm that balances confidentiality and availability by evaluating the user&apos;s impact on resources.&lt;/p&gt;
&lt;h2&gt;Key Contributions&lt;/h2&gt;
&lt;p&gt;Our contributions include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Limitations Analysis&lt;/strong&gt;: Summarizing the limitations of existing risk scoring systems in companies that implement zero trust&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Importance Metric&lt;/strong&gt;: Proposing a dynamic importance metric that measures the importance of resources accessible to users within zero trust systems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Risk-Scoring Algorithm&lt;/strong&gt;: Introducing a risk-scoring algorithm that employs the dynamic importance metric to enhance both security and availability in zero trust environments&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Methodology&lt;/h2&gt;
&lt;p&gt;By incorporating the dynamic importance metric, our proposed algorithm provides a more accurate representation of risk, leading to better security decisions and improved resource availability for legitimate users. This proposal aims to help organizations achieve a more balanced approach to endpoint device security, addressing the unique challenges posed by the increasing prevalence of remote work.&lt;/p&gt;
&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;This research addresses critical security challenges in the post-COVID-19 remote work environment, providing organizations with a more balanced approach to endpoint device security that maintains both security and availability.&lt;/p&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Journal&lt;/strong&gt;: Electronics (MDPI)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Volume&lt;/strong&gt;: 12&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: 8&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 1906&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publisher&lt;/strong&gt;: MDPI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publication Date&lt;/strong&gt;: April 18, 2023&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Park, U.H.; Hong, J.-h.; Kim, A.; Son, K.H. Endpoint Device Risk-Scoring Algorithm Proposal for Zero Trust. Electronics 2023, 12, 1906. &lt;a href=&quot;https://doi.org/10.3390/electronics12081906&quot;&gt;https://doi.org/10.3390/electronics12081906&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Understanding Emotion Changes in Mobile Experience Sampling]]></title><description><![CDATA[Understanding emotion changes in mobile experience sampling Abstract Mobile experience sampling methods (ESMs) are widely used to measureâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Mobile-ESM-emotion-changes/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Mobile-ESM-emotion-changes/</guid><pubDate>Fri, 29 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Understanding emotion changes in mobile experience sampling&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Mobile experience sampling methods (ESMs) are widely used to measure users&apos; affective states by randomly sending self-report requests. However, this random probing can interrupt users and adversely influence users&apos; emotional states by inducing disturbance and stress.&lt;/p&gt;
&lt;p&gt;This work aims to understand how ESMs themselves may compromise the validity of ESM responses and what contextual factors contribute to changes in emotions when users respond to ESMs.&lt;/p&gt;
&lt;h2&gt;Research Methodology&lt;/h2&gt;
&lt;p&gt;Towards this goal, we analyze 2,227 samples of the mobile ESM data collected from 78 participants. Our results show ESM interruptions positively or negatively affected users&apos; emotional states in at least 38% of ESMs, and the changes in emotions are closely related to the contexts users were in prior to ESMs.&lt;/p&gt;
&lt;h2&gt;Key Research Findings&lt;/h2&gt;
&lt;h3&gt;ESM Impact on Emotional States&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interruption Effects&lt;/strong&gt;: ESM interruptions affected users&apos; emotional states in at least 38% of cases&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bidirectional Impact&lt;/strong&gt;: Both positive and negative emotional changes were observed&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context Dependency&lt;/strong&gt;: Emotional changes closely related to users&apos; prior contexts&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Contextual Factors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pre-ESM Context&lt;/strong&gt;: Users&apos; emotional state before receiving ESM requests&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environmental Factors&lt;/strong&gt;: Surrounding conditions and activities&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Temporal Factors&lt;/strong&gt;: Time of day and duration of activities&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Social Context&lt;/strong&gt;: Presence of others and social interactions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Research Contributions&lt;/h2&gt;
&lt;h3&gt;1. ESM Validity Assessment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Measurement Accuracy&lt;/strong&gt;: Understanding how ESMs themselves affect responses&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bias Identification&lt;/strong&gt;: Recognition of potential measurement biases&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Methodological Implications&lt;/strong&gt;: Impact on research validity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Contextual Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pre-interruption State&lt;/strong&gt;: Analysis of users&apos; emotional state before ESM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environmental Context&lt;/strong&gt;: Understanding surrounding conditions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Temporal Patterns&lt;/strong&gt;: Time-based variations in emotional responses&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Mitigation Strategies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Design Considerations&lt;/strong&gt;: Recommendations for reducing ESM interference&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Timing Optimization&lt;/strong&gt;: Optimal moments for ESM delivery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Experience&lt;/strong&gt;: Balancing research needs with user comfort&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Participants&lt;/strong&gt;: 78 participants&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Points&lt;/strong&gt;: 2,227 mobile ESM samples&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Methodology&lt;/strong&gt;: Experience sampling method with mobile devices&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Analysis&lt;/strong&gt;: Quantitative and qualitative assessment of emotional changes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Practical Implications&lt;/h2&gt;
&lt;h3&gt;Research Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ESM Timing&lt;/strong&gt;: Consider optimal moments for sampling requests&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context Awareness&lt;/strong&gt;: Account for users&apos; current emotional and environmental state&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bias Mitigation&lt;/strong&gt;: Strategies to reduce measurement interference&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;User Experience&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Minimal Disruption&lt;/strong&gt;: Design ESMs to minimize emotional disturbance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Sensitivity&lt;/strong&gt;: Adapt sampling based on user context&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stress Reduction&lt;/strong&gt;: Methods to reduce ESM-induced stress&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Technical Approach&lt;/h2&gt;
&lt;h3&gt;Data Collection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mobile ESM&lt;/strong&gt;: Random self-report requests via mobile devices&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Emotional Tracking&lt;/strong&gt;: Measurement of affective states before and after ESM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Recording&lt;/strong&gt;: Documentation of environmental and situational factors&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Analysis Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Statistical Analysis&lt;/strong&gt;: Quantitative assessment of emotional changes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Correlation&lt;/strong&gt;: Relationship between context and emotional impact&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pattern Recognition&lt;/strong&gt;: Identification of factors affecting ESM validity&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conference&lt;/strong&gt;: Proceedings of the 2022 CHI conference on human factors in computing systems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 1-14&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publication Date&lt;/strong&gt;: April 29, 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DOI&lt;/strong&gt;: 10.1145/3491102.3517644&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Kang, S., Park, C.Y., Kim, A., Cha, N., &amp;#x26; Lee, U. (2022). Understanding emotion changes in mobile experience sampling. Proceedings of the 2022 CHI conference on human factors in computing systems, 1-14. &lt;a href=&quot;https://doi.org/10.1145/3491102.3517644&quot;&gt;https://doi.org/10.1145/3491102.3517644&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Developing and Evaluating Low-Cost Level 3 Vehicle Simulator]]></title><description><![CDATA[Developing and Evaluating Low-Cost Level 3 Vehicle Simulator Abstract Conditional autonomous vehicles are becoming popular. As autonomousâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Low-cost-level3-simulator/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Low-cost-level3-simulator/</guid><pubDate>Sat, 01 Jan 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Developing and Evaluating Low-Cost Level 3 Vehicle Simulator&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Conditional autonomous vehicles are becoming popular. As autonomous driving technology advances, drivers can spend more time on secondary tasks rather than driving, and accordingly, in-vehicle services for driver convenience will become more diverse. However, paradoxically, services for driver convenience can increase the accident risk of conditional autonomous vehicles. During autonomous driving, drivers must immediately transfer control (manual driving) when situations occur that the system cannot control, because attention distraction from secondary tasks slows response time. This makes it difficult to ensure driver safety. Therefore, research analyzing the negative effects of such services on drivers should be conducted first. To promote related research, this paper developed a simulator that can simulate autonomous vehicles at low cost, and conducted feasibility and usability tests to determine whether it can be used in actual research.&lt;/p&gt;
&lt;h2&gt;Research Motivation&lt;/h2&gt;
&lt;p&gt;With the popularization of conditional autonomous vehicles, automobiles are changing from a space for transportation to a space that provides convenience. The number of autonomous vehicles is expected to reach approximately 60 million by 2028, and accordingly, it is expected that most vehicles in the near future will have conditional autonomous driving mode applied. Automobile companies such as Tesla have been applying low-level autonomous driving technology to automobiles since 2015, and with the popularization of such conditional autonomous vehicles, the burden of driving is decreasing.&lt;/p&gt;
&lt;p&gt;The International Society of Automotive Engineers (SAE) classifies autonomous vehicles into a total of 6 levels from &apos;Level 0&apos; to &apos;Level 5&apos;, which is the fully autonomous stage. Level 0 has no driving assistance functions. The current level of commercialized autonomous vehicles is Level 2. Up to Level 2, the system assists driving, but from Level 3, the system performs driving so that the vehicle can change lanes, overtake the car in front, or avoid obstacles on its own. However, up to Level 4, autonomous driving technology has limitations and performs conditional autonomous driving. That is, when unexpected situations occur, the system may not respond appropriately. At this time, drivers must receive driving control from the system (takeover) and perform manual driving.&lt;/p&gt;
&lt;h2&gt;Research Objectives&lt;/h2&gt;
&lt;p&gt;This study aims to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Simulator Development&lt;/strong&gt;: Create a low-cost Level 3 autonomous vehicle simulator&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feasibility Assessment&lt;/strong&gt;: Evaluate the simulator&apos;s suitability for research purposes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Usability Testing&lt;/strong&gt;: Conduct tests to determine practical usability&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost-Effective Solution&lt;/strong&gt;: Provide an alternative to expensive commercial simulators&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Technical Implementation&lt;/h2&gt;
&lt;h3&gt;CARLA Platform Integration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open Source Platform&lt;/strong&gt;: Utilized CARLA (Car Learning to Act) for simulation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 3 Environment&lt;/strong&gt;: Simulated SAE Level 3 conditional autonomous driving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom Development&lt;/strong&gt;: Built upon existing CARLA framework&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Simulator Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Autonomous Driving Mode&lt;/strong&gt;: Simulates Level 3 autonomous driving conditions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manual Takeover&lt;/strong&gt;: Enables control transfer from autonomous to manual mode&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secondary Task Support&lt;/strong&gt;: Allows drivers to engage in non-driving activities&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-time Monitoring&lt;/strong&gt;: Tracks driver behavior and response times&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Research Methodology&lt;/h2&gt;
&lt;h3&gt;Development Process&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Platform Selection&lt;/strong&gt;: Chose CARLA as the base simulation platform&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom Implementation&lt;/strong&gt;: Developed Level 3 autonomous driving features&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interface Design&lt;/strong&gt;: Created user-friendly control and monitoring interfaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Testing Framework&lt;/strong&gt;: Established evaluation protocols&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Evaluation Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feasibility Testing&lt;/strong&gt;: Assessed technical capabilities and limitations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Usability Testing&lt;/strong&gt;: Evaluated user experience and interface design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Analysis&lt;/strong&gt;: Measured system reliability and accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Key Findings&lt;/h2&gt;
&lt;h3&gt;Technical Performance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simulation Quality&lt;/strong&gt;: Adequate for research purposes despite some limitations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost Effectiveness&lt;/strong&gt;: Significant cost reduction compared to commercial simulators&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;: Successfully simulates Level 3 autonomous driving conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;User Experience&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Handling Feedback&lt;/strong&gt;: Participants noted unnatural steering sensitivity&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Behavior&lt;/strong&gt;: Unrealistic pedestrian crossing speeds were identified&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overall Usability&lt;/strong&gt;: Simulator deemed suitable for research applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Limitations Identified&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Steering Sensitivity&lt;/strong&gt;: Mismatch between real vehicle and simulator steering response&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Animation&lt;/strong&gt;: Unrealistic pedestrian movement patterns&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environmental Realism&lt;/strong&gt;: Some aspects of the driving environment need improvement&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Experimental Results&lt;/h2&gt;
&lt;h3&gt;Participant Demographics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sample Size&lt;/strong&gt;: 10 participants&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Age Range&lt;/strong&gt;: 20-25 years (mean: 22.3 years)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Response Time&lt;/strong&gt;: 0.763-2.199 seconds (mean: 1.453 seconds)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Performance Metrics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Takeover Response Time&lt;/strong&gt;: Average 1.453 seconds&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;System Reliability&lt;/strong&gt;: Consistent performance across test sessions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Acceptance&lt;/strong&gt;: Positive feedback for research applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Practical Implications&lt;/h2&gt;
&lt;h3&gt;Research Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Driver Behavior Studies&lt;/strong&gt;: Analyze driver responses during autonomous driving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secondary Task Research&lt;/strong&gt;: Study non-driving activities during autonomous mode&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safety Assessment&lt;/strong&gt;: Evaluate the impact of in-vehicle services on safety&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Cost Benefits&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accessibility&lt;/strong&gt;: Enables research without expensive commercial simulators&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Can be deployed across multiple research institutions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Customization&lt;/strong&gt;: Allows modification for specific research needs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Future Improvements&lt;/h2&gt;
&lt;h3&gt;Technical Enhancements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Steering Sensitivity&lt;/strong&gt;: Adjust steering response to match real vehicles&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Behavior&lt;/strong&gt;: Improve pedestrian movement realism&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environmental Details&lt;/strong&gt;: Enhance overall simulation fidelity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Research Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Extended Testing&lt;/strong&gt;: Conduct longer-term studies with more participants&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scenario Development&lt;/strong&gt;: Create diverse driving scenarios for testing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Collection&lt;/strong&gt;: Implement comprehensive data logging capabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conference&lt;/strong&gt;: Proceedings of HCI Korea 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 283-288&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Year&lt;/strong&gt;: 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Location&lt;/strong&gt;: HCIK 2022&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Hwang, J., Han, M., Kim, M., &amp;#x26; Kim, A. (2022). Developing and Evaluating Low-Cost Level 3 Vehicle Simulator. Proceedings of HCI Korea 2022, 283-288.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[A Survey on Design for Development of Voice Driving Assistance Agent]]></title><description><![CDATA[A Survey on Design for Development of Voice Driving Assistance Agent Abstract Road safety and accident prevention are paramount for driversâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Voice-driving-assistance-agent/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Voice-driving-assistance-agent/</guid><pubDate>Sat, 01 Jan 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;A Survey on Design for Development of Voice Driving Assistance Agent&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Road safety and accident prevention are paramount for drivers on the road. To prevent traffic accidents in advance, drivers need to be aware of road conditions, but it is nearly impossible for drivers to be aware of all traffic situations. However, when a passenger is present in the vehicle, they can warn the driver about blind spots that the driver cannot see or unexpected accidents, thereby preventing accidents.&lt;/p&gt;
&lt;p&gt;This study proposes a voice driving assistance agent that provides driving assistance guidance like a passenger, and aims to derive design requirements that should be considered when developing such an agent. To derive design requirements, this study recruited participant teams consisting of passengers and drivers, observed passenger driving guidance during actual road driving, and conducted interviews and surveys on passenger driving assistance guidance during experimental road driving and daily life, deriving three major themes and discussing related design implications.&lt;/p&gt;
&lt;h2&gt;Research Motivation&lt;/h2&gt;
&lt;p&gt;Traffic accidents in Korea are among the highest among OECD countries. Approximately 1.2 million people die and 5 million people are injured in traffic accidents annually. This is not just a problem in Korea. Globally, traffic accidents caused by automobiles are causing serious socio-economic damage.&lt;/p&gt;
&lt;p&gt;Most of these traffic accidents are caused by driver carelessness or poor operation. For example, accidents occur when drivers cannot respond quickly to dangerous situations such as when the car in front suddenly stops or when a pedestrian suddenly appears on a side street.&lt;/p&gt;
&lt;p&gt;Fortunately, having a passenger in the vehicle during dangerous situations helps prevent accidents. According to related studies, when experienced drivers drive with passengers, the risk of accidents is about 1.5 times lower than when driving alone. Similarly, Cooper&apos;s study showed that when comparing novice drivers, those with passengers had lower driver error rates than those without passengers.&lt;/p&gt;
&lt;h2&gt;Research Objectives&lt;/h2&gt;
&lt;p&gt;This study aims to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Design Requirements Investigation&lt;/strong&gt;: Survey design requirements for developing a voice driving assistance agent&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Passenger Behavior Analysis&lt;/strong&gt;: Understand how passengers provide guidance to drivers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Agent Development Guidelines&lt;/strong&gt;: Derive design implications for voice driving assistance agents&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Research Methodology&lt;/h2&gt;
&lt;p&gt;The study recruited participant teams consisting of passengers who usually provide driving guidance to drivers and the corresponding drivers from online communities. The experiment was conducted in the following order: (1) explanation of experimental purpose and procedure, (2) road driving, (3) interview, (4) survey.&lt;/p&gt;
&lt;h3&gt;Data Collection Process&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Road Driving&lt;/strong&gt;: Participants drove along the route presented by the experimenter&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Observation&lt;/strong&gt;: Researchers observed passenger guidance during actual driving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interviews&lt;/strong&gt;: Conducted interviews about provided guidance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Surveys&lt;/strong&gt;: Collected opinions on situations where driving assistance would be helpful&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Key Findings&lt;/h2&gt;
&lt;h3&gt;1. Safety-Related Guidance Situations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hazard Detection&lt;/strong&gt;: Passengers warn about potential dangers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blind Spot Monitoring&lt;/strong&gt;: Alerting drivers to areas they cannot see&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Emergency Situations&lt;/strong&gt;: Providing guidance during unexpected events&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Route-Related Guidance Situations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Navigation Assistance&lt;/strong&gt;: Helping with route planning and directions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Turn-by-Turn Guidance&lt;/strong&gt;: Providing intuitive route change information&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Query Response&lt;/strong&gt;: Answering driver questions about routes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Conflict Resolution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Information Preferences&lt;/strong&gt;: Drivers prefer selective information delivery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Timing Considerations&lt;/strong&gt;: Optimal moments for providing guidance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Customization&lt;/strong&gt;: Allowing drivers to choose preferred guidance types&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Design Implications&lt;/h2&gt;
&lt;h3&gt;1. Natural and Intuitive Communication&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conversational Interface&lt;/strong&gt;: Design agent to communicate like a human passenger&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context Awareness&lt;/strong&gt;: Provide guidance based on current driving situation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Timing Optimization&lt;/strong&gt;: Choose appropriate moments for intervention&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Selective Information Delivery&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Preferences&lt;/strong&gt;: Allow drivers to select preferred guidance types&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conflict Avoidance&lt;/strong&gt;: Prevent providing information drivers already know&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Customization Options&lt;/strong&gt;: Enable personalized guidance settings&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Safety-First Approach&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hazard Detection&lt;/strong&gt;: Focus on safety-critical information&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blind Spot Coverage&lt;/strong&gt;: Monitor areas drivers cannot see&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Emergency Response&lt;/strong&gt;: Provide guidance during dangerous situations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Technical Considerations&lt;/h2&gt;
&lt;h3&gt;Voice Interface Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;: Enable conversational interactions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context Recognition&lt;/strong&gt;: Understand current driving situation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Response Generation&lt;/strong&gt;: Provide appropriate guidance based on context&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;User Experience&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Minimal Disruption&lt;/strong&gt;: Ensure guidance does not interfere with driving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intuitive Interaction&lt;/strong&gt;: Make the interface easy to understand and use&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Personalization&lt;/strong&gt;: Adapt to individual driver preferences&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Future Work&lt;/h2&gt;
&lt;p&gt;The study plans to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Prototype Development&lt;/strong&gt;: Create an agent based on derived design guidelines&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Field Testing&lt;/strong&gt;: Deploy the agent for real-world road testing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effectiveness Evaluation&lt;/strong&gt;: Verify the actual effects on road safety&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conference&lt;/strong&gt;: Proceedings of HCI Korea 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 283-288&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Year&lt;/strong&gt;: 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Location&lt;/strong&gt;: HCIK 2022&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Kim, M., Han, M., Hwang, J., &amp;#x26; Kim, A. (2022). A Survey on Design for Development of Voice Driving Assistance Agent. Proceedings of HCI Korea 2022, 283-288.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Sticky Goals: Understanding Goal Commitments for Behavioral Changes in the Wild]]></title><description><![CDATA[Sticky Goals: Understanding Goal Commitments for Behavioral Changes in the Wild Abstract A commitment device, an attempt to bind oneself forâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Sticky-goals/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Sticky-goals/</guid><pubDate>Thu, 06 May 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Sticky Goals: Understanding Goal Commitments for Behavioral Changes in the Wild&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;A commitment device, an attempt to bind oneself for a successful goal achievement, has been used as an effective strategy to promote behavior change. However, little is known about how commitment devices are used in the wild, and what aspects of commitment devices are related to goal achievements.&lt;/p&gt;
&lt;p&gt;In this paper, we explore a large-scale dataset from stickK, an online behavior change support system that provides both financial and social commitments. We characterize the patterns of behavior change goals (e.g., topics and commitment setting) and then perform a series of multilevel regression analyses on goal achievements.&lt;/p&gt;
&lt;h2&gt;Key Findings&lt;/h2&gt;
&lt;p&gt;Our results reveal that successful goal achievements are largely dependent on the configuration of financial and social commitment devices, and a mixed commitment setting is considered beneficial. We discuss how our findings could inform the design of effective commitment devices.&lt;/p&gt;
&lt;h2&gt;Research Approach&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: Large-scale data from stickK, an online behavior change support system&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Analysis&lt;/strong&gt;: Multilevel regression analyses on goal achievements&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Focus&lt;/strong&gt;: Financial and social commitment devices&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method&lt;/strong&gt;: Characterization of behavior change goal patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Key Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pattern Analysis&lt;/strong&gt;: Characterization of behavior change goals (topics and commitment setting)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistical Analysis&lt;/strong&gt;: Multilevel regression analyses on goal achievements&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Design Implications&lt;/strong&gt;: Insights for effective commitment device design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-world Understanding&lt;/strong&gt;: Analysis of commitment devices usage in the wild&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;This research provides valuable insights into how commitment devices work in real-world settings, informing the design of more effective behavior change support systems that can help users achieve their goals through strategic use of financial and social commitments.&lt;/p&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conference&lt;/strong&gt;: CHI 2021 - ACM Conference on Human Factors in Computing Systems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 1-16&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publication Date&lt;/strong&gt;: May 6, 2021&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publisher&lt;/strong&gt;: ACM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Lee, H., Kim, A., Hong, H., &amp;#x26; Lee, U. (2021). Sticky goals: understanding goal commitments for behavioral changes in the wild. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-16). &lt;a href=&quot;https://doi.org/10.1145/3411764.3445760&quot;&gt;https://doi.org/10.1145/3411764.3445760&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Benefits of Mobile Contact Tracing on COVID-19: Tracing Capacity Perspectives]]></title><description><![CDATA[Benefits of Mobile Contact Tracing on COVID-19: Tracing Capacity Perspectives Abstract For effectively suppressing COVID-19's spreadâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Mobile-contact-tracing/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Mobile-contact-tracing/</guid><pubDate>Thu, 18 Mar 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Benefits of Mobile Contact Tracing on COVID-19: Tracing Capacity Perspectives&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;For effectively suppressing COVID-19&apos;s spread, contact tracing has been widely used to identify, isolate, and follow-up with those who have come in close contact with an infected person (or &quot;close contacts&quot;). Traditionally, contact tracers in local health offices interview an infected person to identify visited places (or hotspots) and then check any close contacts.&lt;/p&gt;
&lt;p&gt;For the accurate recall of travel history, several countries including South Korea corroborate multiple data sources such as cell location or credit card transactions. Beside this traditional approach, various mobile apps were introduced to help improve travel history tracking including automated GPS tracking (e.g., Israel&apos;s HaMagen) and manual place QR-code scanning (e.g., New Zealand&apos;s NZ COVID Tracer and Korea&apos;s KI Pass).&lt;/p&gt;
&lt;h2&gt;Mobile Contact Tracing Approaches&lt;/h2&gt;
&lt;p&gt;Alternatively, mobile apps maintain individuals&apos; &quot;encounter history&quot; (instead of place visit history) by leveraging peer-to-peer wireless beaconing (i.e., self-announcing its presence to nearby devices) with Bluetooth Low Energy (BLE) in smartphones such as Google-Apple&apos;s Exposure Notification and Singapore&apos;s BlueTrace and TraceTogether. This encounter history can be used later to judge whether a user had a risky encounter with an infected person.&lt;/p&gt;
&lt;h2&gt;Key Argument&lt;/h2&gt;
&lt;p&gt;We argue that traditional manual contact tracing can be greatly improved by leveraging the wisdom of crowds. Local community members install mobile apps to self-collect &quot;breadcrumbs&quot; for contact tracing such as GPS traces, place QR-codes, and wireless encounter histories, which can offer near real-time assessment.&lt;/p&gt;
&lt;h2&gt;Research Focus&lt;/h2&gt;
&lt;p&gt;However, there is a systematic lack of adoption of mobile apps in many countries, and this research explores the benefits and challenges of mobile contact tracing from tracing capacity perspectives.&lt;/p&gt;
&lt;h2&gt;Key Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Traditional vs. Mobile Tracing&lt;/strong&gt;: Comparison of traditional manual contact tracing with mobile-based approaches&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Technology Analysis&lt;/strong&gt;: Examination of GPS tracking, QR-code scanning, and BLE beaconing methods&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capacity Perspectives&lt;/strong&gt;: Analysis of tracing capacity improvements through mobile technology&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adoption Challenges&lt;/strong&gt;: Investigation of systematic barriers to mobile app adoption&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;This research provides critical insights into how mobile technology can enhance COVID-19 contact tracing efforts, offering practical guidance for public health officials and policymakers on implementing effective digital contact tracing solutions.&lt;/p&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Journal&lt;/strong&gt;: Frontiers in Public Health&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Volume&lt;/strong&gt;: 9&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 586615&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publisher&lt;/strong&gt;: Frontiers Media SA&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publication Date&lt;/strong&gt;: March 18, 2021&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Lee, U., &amp;#x26; Kim, A. (2021). Benefits of mobile contact tracing on COVID-19: tracing capacity perspectives. Frontiers in Public Health, 9, 586615. &lt;a href=&quot;https://doi.org/10.3389/fpubh.2021.586615&quot;&gt;https://doi.org/10.3389/fpubh.2021.586615&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Towards Flow Control of Driver-Vehicle Interactions]]></title><description><![CDATA[Towards Flow Control of Driver-Vehicle Interactions Abstract This paper explores the concept of flow control in driver-vehicle interactionsâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Driver-vehicle-flow-control/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Driver-vehicle-flow-control/</guid><pubDate>Wed, 21 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Towards Flow Control of Driver-Vehicle Interactions&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper explores the concept of flow control in driver-vehicle interactions, focusing on how to optimize the interaction between drivers and autonomous or semi-autonomous vehicles to enhance safety, efficiency, and user experience.&lt;/p&gt;
&lt;h2&gt;Research Focus&lt;/h2&gt;
&lt;p&gt;The research investigates methods to control and optimize the flow of interactions between drivers and vehicles, particularly in the context of autonomous driving systems and human-computer interaction in automotive environments.&lt;/p&gt;
&lt;h2&gt;Key Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Flow Control Framework&lt;/strong&gt;: Development of a framework for managing driver-vehicle interaction flows&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interaction Optimization&lt;/strong&gt;: Methods for optimizing the timing and nature of driver-vehicle communications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safety Enhancement&lt;/strong&gt;: Strategies for improving safety through better interaction control&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Experience&lt;/strong&gt;: Approaches to enhance driver experience through flow control&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Conference Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conference&lt;/strong&gt;: International Conference on ICT Convergence (ICTC 2020)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date&lt;/strong&gt;: October 21, 2020&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Uichin Lee, Auk Kim, Jungmi Park, Woohyeok Choi&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;This research contributes to the field of automotive human-computer interaction by providing insights into how to better control and optimize the flow of interactions between drivers and vehicles, ultimately leading to safer and more efficient driving experiences.&lt;/p&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Lee, U., Kim, A., Park, J., &amp;#x26; Choi, W. (2020). Towards Flow Control of Driver-Vehicle Interactions. In International Conference on ICT Convergence (ICTC 2020).&lt;/p&gt;</content:encoded></item><item><title><![CDATA[K-EmoCon, a Multimodal Sensor Dataset for Continuous Emotion Recognition in Naturalistic Conversations]]></title><description><![CDATA[K-EmoCon, a Multimodal Sensor Dataset for Continuous Emotion Recognition in Naturalistic Conversations Abstract Recognizing emotions duringâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/K-EmoCon/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/K-EmoCon/</guid><pubDate>Tue, 08 Sep 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;K-EmoCon, a Multimodal Sensor Dataset for Continuous Emotion Recognition in Naturalistic Conversations&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Recognizing emotions during social interactions has many potential applications with the popularization of low-cost mobile sensors, but a challenge remains with the lack of naturalistic affective interaction data. Most existing emotion datasets do not support studying idiosyncratic emotions arising in the wild as they were collected in constrained environments.&lt;/p&gt;
&lt;p&gt;Therefore, studying emotions in the context of social interactions requires a novel dataset, and K-EmoCon is such a multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations.&lt;/p&gt;
&lt;h2&gt;Dataset Overview&lt;/h2&gt;
&lt;p&gt;The dataset contains multimodal measurements, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Audiovisual recordings&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EEG (Electroencephalography)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Peripheral physiological signals&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All data was acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue.&lt;/p&gt;
&lt;h2&gt;Key Features&lt;/h2&gt;
&lt;p&gt;Distinct from previous datasets, K-EmoCon includes emotion annotations from all three available perspectives, providing comprehensive coverage of emotional states during naturalistic conversations.&lt;/p&gt;
&lt;h2&gt;Research Applications&lt;/h2&gt;
&lt;p&gt;This dataset enables research in:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Continuous emotion recognition&lt;/strong&gt; in naturalistic settings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multimodal emotion analysis&lt;/strong&gt; using various sensor data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Social interaction emotion modeling&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-world affective computing&lt;/strong&gt; applications&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Dataset Specifications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sessions&lt;/strong&gt;: 16 debate sessions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Duration&lt;/strong&gt;: ~10 minutes per session&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participants&lt;/strong&gt;: Paired debates on social issues&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensors&lt;/strong&gt;: Off-the-shelf devices for accessibility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Annotations&lt;/strong&gt;: Comprehensive emotion labels from multiple perspectives&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;K-EmoCon addresses the critical gap in naturalistic emotion datasets, enabling researchers to study emotions in real-world social interactions rather than constrained laboratory settings. This dataset has been widely cited (197 citations) and has significantly advanced the field of affective computing.&lt;/p&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Journal&lt;/strong&gt;: Scientific Data (Nature)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Volume&lt;/strong&gt;: 7&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: 1&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publisher&lt;/strong&gt;: Nature Publishing Group&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publication Date&lt;/strong&gt;: September 8, 2020&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Park, C.Y., Cha, N., Kang, S. et al. K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations. Sci Data 7, 285 (2020). &lt;a href=&quot;https://doi.org/10.1038/s41597-020-00630-y&quot;&gt;https://doi.org/10.1038/s41597-020-00630-y&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Hello there! is now a good time to talk? Opportune moments for proactive interactions with smart speakers]]></title><description><![CDATA[Hello there! is now a good time to talk? Opportune moments for proactive interactions with smart speakers Abstract Increasing number ofâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/Smart-speaker-timing/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/Smart-speaker-timing/</guid><pubDate>Fri, 04 Sep 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Hello there! is now a good time to talk? Opportune moments for proactive interactions with smart speakers&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Increasing number of researchers and designers are envisioning a wide range of novel proactive conversational services for smart speakers such as context-aware reminders and restocking household items. When initiating conversational interactions proactively, smart speakers need to consider users&apos; contexts to minimize disruption.&lt;/p&gt;
&lt;p&gt;In this work, we aim to broaden our understanding of opportune moments for proactive conversational interactions in domestic contexts. Toward this goal, we built a voice-based experience sampling device and conducted a one-week field study with 40 participants living in university dormitories.&lt;/p&gt;
&lt;h2&gt;Research Methodology&lt;/h2&gt;
&lt;p&gt;From 3,572 in-situ user experience reports, we proposed 19 activity categories to investigate contextual factors related to interruptibility. Our data analysis results show that the key determinants for opportune moments are closely related to both personal contextual factors such as busyness and environmental factors.&lt;/p&gt;
&lt;h2&gt;Key Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Voice-based Experience Sampling&lt;/strong&gt;: Development of a novel device for collecting in-situ user experience data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Field Study&lt;/strong&gt;: One-week study with 40 participants in university dormitories&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Activity Categorization&lt;/strong&gt;: 19 activity categories for understanding interruptibility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Analysis&lt;/strong&gt;: Investigation of personal and environmental factors affecting opportune moments&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Research Findings&lt;/h2&gt;
&lt;p&gt;The study revealed that opportune moments for proactive interactions are determined by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Personal contextual factors&lt;/strong&gt; (e.g., busyness, emotional state)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environmental factors&lt;/strong&gt; (e.g., location, time of day)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Activity type&lt;/strong&gt; and current engagement level&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Social context&lt;/strong&gt; and presence of others&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Participants&lt;/strong&gt;: 40 university dormitory residents&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Duration&lt;/strong&gt;: One-week field study&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Points&lt;/strong&gt;: 3,572 in-situ user experience reports&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Categories&lt;/strong&gt;: 19 activity categories for analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;This research provides crucial insights for designing proactive conversational services that minimize disruption while maximizing user engagement. The findings inform the development of context-aware smart speaker interactions that respect users&apos; current situations and preferences.&lt;/p&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Journal&lt;/strong&gt;: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Volume&lt;/strong&gt;: 4&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: 3&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 1-28&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publisher&lt;/strong&gt;: ACM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publication Date&lt;/strong&gt;: September 4, 2020&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Cha, N., Kim, A., Park, C.Y., Kang, S., Park, M., Lee, J.G., Lee, S., &amp;#x26; Lee, U. (2020). Hello there! is now a good time to talk? Opportune moments for proactive interactions with smart speakers. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), 1-28. &lt;a href=&quot;https://doi.org/10.1145/3411840&quot;&gt;https://doi.org/10.1145/3411840&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Too Much Information: Assessing Privacy Risks of Contact Trace Data Disclosure on People With COVID-19 in South Korea]]></title><description><![CDATA[Too Much Information: Assessing Privacy Risks of Contact Trace Data Disclosure on People With COVID-19 in South Korea Abstract With theâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/COVID-contact-trace-privacy/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/COVID-contact-trace-privacy/</guid><pubDate>Thu, 18 Jun 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Too Much Information: Assessing Privacy Risks of Contact Trace Data Disclosure on People With COVID-19 in South Korea&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;With the COVID-19 outbreak, South Korea has been making contact trace data public to help people self-check if they have been in contact with a person infected with the coronavirus. Despite its benefits in suppressing the spread of the virus, publicizing contact trace data raises concerns about individuals&apos; privacy.&lt;/p&gt;
&lt;p&gt;In view of this tug-of-war between one&apos;s privacy and public safety, this work aims to deepen the understanding of privacy risks of contact trace data disclosure practices in South Korea.&lt;/p&gt;
&lt;h2&gt;Research Methodology&lt;/h2&gt;
&lt;p&gt;In this study, publicly available contact trace data of 970 confirmed patients were collected from seven metropolitan cities in South Korea (20th Janâ€“20th Apr 2020). Then, an ordinal scale of relative privacy risk levels was introduced for evaluation, and the assessment was performed on the personal information included in the contact trace data, such as demographics, significant places, sensitive information, social relationships, and routine behaviors.&lt;/p&gt;
&lt;p&gt;In addition, variance of privacy risk levels was examined across regions and over time to check for differences in policy implementation.&lt;/p&gt;
&lt;h2&gt;Key Findings&lt;/h2&gt;
&lt;h3&gt;Data Disclosure Patterns&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Demographics&lt;/strong&gt;: Most contact trace data showed the gender and age of the patients&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Significant Places&lt;/strong&gt;: Disclosed home/work locations ranging across different levels of privacy risks in over 70% of the cases&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensitive Information&lt;/strong&gt;: Inference on hobby, religion was made possible&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Social Relationships&lt;/strong&gt;: 48.7% of the cases exposed the patient&apos;s social relationships&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Regional and Temporal Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regional Differences&lt;/strong&gt;: Considerable discrepancy was found in the privacy risk for each category across different regions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Policy Implementation&lt;/strong&gt;: Despite the recent release of government guidelines on data disclosure, its effects varied significantly&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Research Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Privacy Risk Assessment Framework&lt;/strong&gt;: Development of an ordinal scale for evaluating privacy risks in contact trace data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comprehensive Data Analysis&lt;/strong&gt;: Analysis of 970 confirmed patients&apos; data from seven metropolitan cities&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-dimensional Privacy Analysis&lt;/strong&gt;: Assessment across demographics, locations, sensitive information, and social relationships&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Policy Impact Evaluation&lt;/strong&gt;: Examination of regional differences and temporal changes in data disclosure practices&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Privacy Risk Categories&lt;/h2&gt;
&lt;p&gt;The study evaluated privacy risks across several categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Demographics&lt;/strong&gt; (gender, age)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Significant Places&lt;/strong&gt; (home, work locations)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensitive Information&lt;/strong&gt; (hobbies, religion)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Social Relationships&lt;/strong&gt; (family, friends, colleagues)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Routine Behaviors&lt;/strong&gt; (daily patterns, activities)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scope&lt;/strong&gt;: Seven metropolitan cities in South Korea&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Period&lt;/strong&gt;: January 20 - April 20, 2020&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample Size&lt;/strong&gt;: 970 confirmed COVID-19 patients&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Type&lt;/strong&gt;: Publicly available contact trace data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Policy Implications&lt;/h2&gt;
&lt;p&gt;The research highlights the tension between public health benefits and individual privacy rights in the context of COVID-19 contact tracing. The findings suggest the need for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Balanced Disclosure Policies&lt;/strong&gt;: Guidelines that protect privacy while maintaining public health benefits&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regional Standardization&lt;/strong&gt;: Consistent implementation of privacy protection measures across regions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Risk-based Approaches&lt;/strong&gt;: Tailored disclosure practices based on privacy risk assessments&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Journal&lt;/strong&gt;: Frontiers in Public Health&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Volume&lt;/strong&gt;: 8&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 305&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publisher&lt;/strong&gt;: Frontiers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publication Date&lt;/strong&gt;: June 18, 2020&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Jung, G., Lee, H., Kim, A., &amp;#x26; Lee, U. (2020). Too Much Information: Assessing Privacy Risks of Contact Trace Data Disclosure on People With COVID-19 in South Korea. Frontiers in Public Health, 8, 305. &lt;a href=&quot;https://doi.org/10.3389/fpubh.2020.00305&quot;&gt;https://doi.org/10.3389/fpubh.2020.00305&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Interruptibility for in-vehicle multitasking: influence of voice task demands and adaptive behaviors]]></title><description><![CDATA[Interruptibility for in-vehicle multitasking: influence of voice task demands and adaptive behaviors Abstract As a countermeasure to visualâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/In-vehicle-interruptibility/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/In-vehicle-interruptibility/</guid><pubDate>Wed, 18 Mar 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Interruptibility for in-vehicle multitasking: influence of voice task demands and adaptive behaviors&lt;/h1&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As a countermeasure to visual-manual distractions, auditory-verbal (voice) interfaces are becoming increasingly popular for in-vehicle systems. This opens up new opportunities for drivers to receive proactive personalized services from various service domains.&lt;/p&gt;
&lt;p&gt;However, prior studies warned that such interactions can cause cognitive distractions due to the nature of concurrent multitasking with a limited amount of cognitive resources.&lt;/p&gt;
&lt;h2&gt;Research Objectives&lt;/h2&gt;
&lt;p&gt;In this study, we examined:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Task Demand Impact&lt;/strong&gt;: How the varying demands of proactive voice tasks under diverse driving situations impact driver interruptibility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adaptive Behavior Analysis&lt;/strong&gt;: How drivers adapt their concurrent multitasking of driving and proactive voice tasks, and how the adaptive behaviors are related to driver interruptibility&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Research Methodology&lt;/h2&gt;
&lt;p&gt;Our quantitative and qualitative analyses showed that in addition to the driving-task demand, the voice-task demand and adaptive behaviors are also significantly related to driver interruptibility.&lt;/p&gt;
&lt;h2&gt;Key Research Contributions&lt;/h2&gt;
&lt;h3&gt;1. Voice Interface Integration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt;: In-vehicle systems increasingly adopting auditory-verbal interfaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Motivation&lt;/strong&gt;: Countermeasure to visual-manual distractions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Opportunity&lt;/strong&gt;: Proactive personalized services from various domains&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Cognitive Load Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: Cognitive distractions from concurrent multitasking&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Constraint&lt;/strong&gt;: Limited cognitive resources&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Focus&lt;/strong&gt;: Balance between driving safety and voice interaction efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Interruptibility Assessment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Primary Factor&lt;/strong&gt;: Driving-task demand&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secondary Factors&lt;/strong&gt;: Voice-task demand and adaptive behaviors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Methodology&lt;/strong&gt;: Quantitative and qualitative analysis approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Research Findings&lt;/h2&gt;
&lt;h3&gt;Driving-Task Demand&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Primary Influence&lt;/strong&gt;: Significantly impacts driver interruptibility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt;: Various driving situations and conditions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Measurement&lt;/strong&gt;: Quantitative assessment of driving complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Voice-Task Demand&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Secondary Influence&lt;/strong&gt;: Also significantly related to interruptibility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variability&lt;/strong&gt;: Different levels of cognitive load from voice interactions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integration&lt;/strong&gt;: Combined effect with driving-task demand&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Adaptive Behaviors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Driver Strategies&lt;/strong&gt;: How drivers manage concurrent tasks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Behavioral Patterns&lt;/strong&gt;: Identification of effective multitasking strategies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: Direct correlation with interruptibility levels&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Practical Implications&lt;/h2&gt;
&lt;h3&gt;Vehicle Interface Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Voice System Optimization&lt;/strong&gt;: Design considerations for cognitive load management&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proactive Service Timing&lt;/strong&gt;: Optimal moments for voice interaction initiation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safety Integration&lt;/strong&gt;: Balancing convenience with driving safety&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Driver Behavior Understanding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Adaptive Strategy Recognition&lt;/strong&gt;: Identifying effective multitasking approaches&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training Opportunities&lt;/strong&gt;: Potential for driver education on safe voice interaction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Personalization&lt;/strong&gt;: Tailoring voice interfaces to individual driver capabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Technical Approach&lt;/h2&gt;
&lt;h3&gt;Quantitative Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Collection&lt;/strong&gt;: Systematic measurement of task demands and interruptibility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistical Modeling&lt;/strong&gt;: Relationship analysis between variables&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Metrics&lt;/strong&gt;: Objective assessment of driver performance&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Qualitative Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Behavioral Observation&lt;/strong&gt;: In-depth analysis of driver adaptation strategies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pattern Recognition&lt;/strong&gt;: Identification of effective multitasking behaviors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Experience&lt;/strong&gt;: Subjective assessment of voice interaction quality&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Publication Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Journal&lt;/strong&gt;: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Volume&lt;/strong&gt;: 4&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: 1&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 1-22&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publisher&lt;/strong&gt;: ACM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publication Date&lt;/strong&gt;: March 18, 2020&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;Kim, A., Park, J.M., &amp;#x26; Lee, U. (2020). Interruptibility for in-vehicle multitasking: influence of voice task demands and adaptive behaviors. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(1), 1-22. &lt;a href=&quot;https://doi.org/10.1145/3381014&quot;&gt;https://doi.org/10.1145/3381014&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Hello World]]></title><description><![CDATA[This is my first post on my new fake blog! How exciting! I'm sure I'll write a lot more interesting things in the future. Oh, and here's aâ€¦]]></description><link>https://hai-lab-knu.github.io/HAI-Lab-Website/hello-world/</link><guid isPermaLink="false">https://hai-lab-knu.github.io/HAI-Lab-Website/hello-world/</guid><pubDate>Fri, 01 May 2015 22:12:03 GMT</pubDate><content:encoded>&lt;p&gt;This is my first post on my new fake blog! How exciting!&lt;/p&gt;
&lt;p&gt;I&apos;m sure I&apos;ll write a lot more interesting things in the future.&lt;/p&gt;
&lt;p&gt;Oh, and here&apos;s a great quote from this Wikipedia on
&lt;a href=&quot;https://en.wikipedia.org/wiki/Salted_duck_egg&quot;&gt;salted duck eggs&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A salted duck egg is a Chinese preserved food product made by soaking duck
eggs in brine, or packing each egg in damp, salted charcoal. In Asian
supermarkets, these eggs are sometimes sold covered in a thick layer of salted
charcoal paste. The eggs may also be sold with the salted paste removed,
wrapped in plastic, and vacuum packed. From the salt curing process, the
salted duck eggs have a briny aroma, a gelatin-like egg white and a
firm-textured, round yolk that is bright orange-red in color.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/8058f3f26913fea3b6a89a73344fe94a/e1596/salty_egg.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75.31645569620254%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMB/8QAFwEAAwEAAAAAAAAAAAAAAAAAAAEEBf/aAAwDAQACEAMQAAABgik0dXC//8QAGRABAAMBAQAAAAAAAAAAAAAAAQACERIy/9oACAEBAAEFAkqW7B5Zox9t8n//xAAXEQEAAwAAAAAAAAAAAAAAAAAAERJB/9oACAEDAQE/AdVl/8QAFhEBAQEAAAAAAAAAAAAAAAAAABEC/9oACAECAQE/AYun/8QAGxAAAQQDAAAAAAAAAAAAAAAAAAECESEiMTL/2gAIAQEABj8CvRjaHMkvIQo//8QAGxAAAwEBAAMAAAAAAAAAAAAAAREhAEGBobH/2gAIAQEAAT8hsTTH203YINmYDPHckkPzQiAnv//aAAwDAQACAAMAAAAQyC//xAAXEQEBAQEAAAAAAAAAAAAAAAABABEh/9oACAEDAQE/EBHDK6y//8QAGBEAAgMAAAAAAAAAAAAAAAAAAAERIVH/2gAIAQIBAT8QWiS0j//EABwQAQEAAgMBAQAAAAAAAAAAAAERADEhYYFB4f/aAAgBAQABPxBOzi0NHmChLtHkL9EcWZeCVJ1kcRoVp3lEYcnjEk0ES7/Jn//Z&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Chinese Salty Egg&quot;
        title=&quot;&quot;
        src=&quot;/static/8058f3f26913fea3b6a89a73344fe94a/828fb/salty_egg.jpg&quot;
        srcset=&quot;/static/8058f3f26913fea3b6a89a73344fe94a/ff44c/salty_egg.jpg 158w,
/static/8058f3f26913fea3b6a89a73344fe94a/a6688/salty_egg.jpg 315w,
/static/8058f3f26913fea3b6a89a73344fe94a/828fb/salty_egg.jpg 630w,
/static/8058f3f26913fea3b6a89a73344fe94a/0ede0/salty_egg.jpg 945w,
/static/8058f3f26913fea3b6a89a73344fe94a/3ac88/salty_egg.jpg 1260w,
/static/8058f3f26913fea3b6a89a73344fe94a/e1596/salty_egg.jpg 2048w&quot;
        sizes=&quot;(max-width: 630px) 100vw, 630px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You can also write code blocks here!&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;js&quot;&gt;&lt;pre class=&quot;language-js&quot;&gt;&lt;code class=&quot;language-js&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;const&lt;/span&gt; saltyDuckEgg &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;chinese preserved food product&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;left&quot;&gt;Number&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Title&lt;/th&gt;
&lt;th align=&quot;right&quot;&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Harry Potter and the Philosopherâ€™s Stone&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;2001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Harry Potter and the Chamber of Secrets&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;2002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Harry Potter and the Prisoner of Azkaban&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;2004&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href=&quot;https://raw.github.com/adamschwartz/github-markdown-kitchen-sink/master/README.md&quot;&gt;View raw (TEST.md)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a paragraph.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;This is a paragraph.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Header 1&lt;/h1&gt;
&lt;h2&gt;Header 2&lt;/h2&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Header 1
========

Header 2
--------&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Header 1&lt;/h1&gt;
&lt;h2&gt;Header 2&lt;/h2&gt;
&lt;h3&gt;Header 3&lt;/h3&gt;
&lt;h4&gt;Header 4&lt;/h4&gt;
&lt;h5&gt;Header 5&lt;/h5&gt;
&lt;h6&gt;Header 6&lt;/h6&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;# Header 1
## Header 2
### Header 3
#### Header 4
##### Header 5
###### Header 6&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Header 1&lt;/h1&gt;
&lt;h2&gt;Header 2&lt;/h2&gt;
&lt;h3&gt;Header 3&lt;/h3&gt;
&lt;h4&gt;Header 4&lt;/h4&gt;
&lt;h5&gt;Header 5&lt;/h5&gt;
&lt;h6&gt;Header 6&lt;/h6&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;# Header 1 #
## Header 2 ##
### Header 3 ###
#### Header 4 ####
##### Header 5 #####
###### Header 6 ######&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&gt; Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;h2&gt;This is a header.&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;This is the first list item.&lt;/li&gt;
&lt;li&gt;This is the second list item.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here&apos;s some example code:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Markdown.generate();&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&gt; ## This is a header.
&gt; 1. This is the first list item.
&gt; 2. This is the second list item.
&gt;
&gt; Here&apos;s some example code:
&gt;
&gt;     Markdown.generate();&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;markdown&quot;&gt;&lt;pre class=&quot;language-markdown&quot;&gt;&lt;code class=&quot;language-markdown&quot;&gt;&lt;span class=&quot;token list punctuation&quot;&gt;-&lt;/span&gt; Red
&lt;span class=&quot;token list punctuation&quot;&gt;-&lt;/span&gt; Green
&lt;span class=&quot;token list punctuation&quot;&gt;-&lt;/span&gt; Blue

&lt;span class=&quot;token list punctuation&quot;&gt;*&lt;/span&gt; Red
&lt;span class=&quot;token list punctuation&quot;&gt;*&lt;/span&gt; Green
&lt;span class=&quot;token list punctuation&quot;&gt;*&lt;/span&gt; Blue

&lt;span class=&quot;token list punctuation&quot;&gt;-&lt;/span&gt; Red
&lt;span class=&quot;token list punctuation&quot;&gt;-&lt;/span&gt; Green
&lt;span class=&quot;token list punctuation&quot;&gt;-&lt;/span&gt; Blue&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;code goes&lt;/code&gt; here in this line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bold&lt;/strong&gt; goes here&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;markdown&quot;&gt;&lt;pre class=&quot;language-markdown&quot;&gt;&lt;code class=&quot;language-markdown&quot;&gt;&lt;span class=&quot;token list punctuation&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;token code-snippet code keyword&quot;&gt;`code goes`&lt;/span&gt; here in this line
&lt;span class=&quot;token list punctuation&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;token bold&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;token content&quot;&gt;bold&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;**&lt;/span&gt;&lt;/span&gt; goes here&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Buy flour and salt&lt;/li&gt;
&lt;li&gt;Mix together with water&lt;/li&gt;
&lt;li&gt;Bake&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;markdown&quot;&gt;&lt;pre class=&quot;language-markdown&quot;&gt;&lt;code class=&quot;language-markdown&quot;&gt;&lt;span class=&quot;token list punctuation&quot;&gt;1.&lt;/span&gt; Buy flour and salt
&lt;span class=&quot;token list punctuation&quot;&gt;1.&lt;/span&gt; Mix together with water
&lt;span class=&quot;token list punctuation&quot;&gt;1.&lt;/span&gt; Bake&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;code goes&lt;/code&gt; here in this line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bold&lt;/strong&gt; goes here&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;markdown&quot;&gt;&lt;pre class=&quot;language-markdown&quot;&gt;&lt;code class=&quot;language-markdown&quot;&gt;&lt;span class=&quot;token list punctuation&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;token code-snippet code keyword&quot;&gt;`code goes`&lt;/span&gt; here in this line
&lt;span class=&quot;token list punctuation&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;token bold&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;token content&quot;&gt;bold&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;**&lt;/span&gt;&lt;/span&gt; goes here&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Paragraph:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Code&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!-- --&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Paragraph:

    Code&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;* * *

***

*****

- - -

---------------------------------------&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is &lt;a href=&quot;http://example.com&quot; title=&quot;Example&quot;&gt;an example&lt;/a&gt; link.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://example.com&quot;&gt;This link&lt;/a&gt; has no title attr.&lt;/p&gt;
&lt;p&gt;This is &lt;a href=&quot;http://example.com&quot; title=&quot;Optional Title&quot;&gt;an example&lt;/a&gt; reference-style link.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;This is [an example](http://example.com &quot;Example&quot;) link.

[This link](http://example.com) has no title attr.

This is [an example] [id] reference-style link.

[id]: http://example.com &quot;Optional Title&quot;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;single asterisks&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;single underscores&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;double asterisks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;double underscores&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;*single asterisks*

_single underscores_

**double asterisks**

__double underscores__&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This paragraph has some &lt;code class=&quot;language-text&quot;&gt;code&lt;/code&gt; in it.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;This paragraph has some `code` in it.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://via.placeholder.com/200x50&quot; alt=&quot;Alt Text&quot; title=&quot;Image Title&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;![Alt Text](https://via.placeholder.com/200x50 &quot;Image Title&quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item></channel></rss>